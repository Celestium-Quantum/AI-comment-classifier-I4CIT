{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9300356,"sourceType":"datasetVersion","datasetId":5631196}],"dockerImageVersionId":30761,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression, Ridge\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import mean_squared_error\nfrom wordcloud import WordCloud\n\n# Load the dataset from Kaggle's environment\n# Kaggle datasets are usually accessed from /kaggle/input/[dataset-name] directory\ndf = pd.read_csv('/kaggle/input/ai-human/AI_Human.csv')\n\n# Display the first few rows of the dataset\nprint(df.head())\n\n# Count the number of 0s and 1s in the 'generated' column\ncounts = df['generated'].value_counts()\nnum_zeros = counts.get(0, 0)\nnum_ones = counts.get(1, 0)\n\nprint('------------------------------------------------')\n\n# Display the statistics\nprint(f\"Number of 0s in 'generated' column: {num_zeros}\")\nprint(f\"Number of 1s in 'generated' column: {num_ones}\")\n\n# Extract features and labels\ncomments = df['text'].astype(str).tolist()\nlabels = df['generated'].tolist()\n\nprint('------------------------------------------------')\n\n# Bar plot for distribution of 0s and 1s in the 'generated' column\nsns.countplot(x='generated', data=df)\nplt.title('Distribution of Generated Labels')\nplt.xlabel('Generated')\nplt.ylabel('Count')\nplt.show()\n\n# Calculate the length of each text entry\ndf['text_length'] = df['text'].apply(len)\n\n# Calculate the average length of the text column\naverage_length = df['text_length'].mean()\n\n# Display the average length\nprint(f\"Average length of text in the 'text' column: {average_length}\")\n\n# Suppress the specific FutureWarning\nwarnings.filterwarnings('ignore', category=FutureWarning, message=\"use_inf_as_na option is deprecated\")\n\n# Histogram of text lengths\nplt.figure(figsize=(10, 6))\nsns.histplot(df['text_length'], bins=50, kde=True)\nplt.title('Distribution of Text Lengths')\nplt.xlabel('Text Length')\nplt.ylabel('Frequency')\nplt.show()\n\n# Box plot of text lengths by generated label\nplt.figure(figsize=(10, 6))\nsns.boxplot(x='generated', y='text_length', data=df)\nplt.title('Box Plot of Text Lengths by Generated Label')\nplt.xlabel('Generated')\nplt.ylabel('Text Length')\nplt.show()\n\n# Violin plot of text lengths by generated label\nplt.figure(figsize=(10, 6))\nsns.violinplot(x='generated', y='text_length', data=df)\nplt.title('Violin Plot of Text Lengths by Generated Label')\nplt.xlabel('Generated')\nplt.ylabel('Text Length')\nplt.show()\n\n# Vectorize the text data\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(comments)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n\n# Train a simple model\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred, zero_division=0))\n\n# RANDOM FOREST CLASSIFIER\nprint(\"------------------------------------------------\")\nprint(\"Random Forest Classifier\")\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\ny_pred_rf = rf_model.predict(X_test)\nrf_accuracy = accuracy_score(y_test, y_pred_rf)\nprint(f\"Random Forest Accuracy: {rf_accuracy}\")\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred_rf, zero_division=0))\n\n# SUPPORT VECTOR MACHINE (SVM)\nprint(\"------------------------------------------------\")\nprint(\"Support Vector Machine (SVM)\")\nsvm_model = SVC(kernel='linear', random_state=42)\nsvm_model.fit(X_train, y_train)\ny_pred_svm = svm_model.predict(X_test)\nsvm_accuracy = accuracy_score(y_test, y_pred_svm)\nprint(f\"SVM Accuracy: {svm_accuracy}\")\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred_svm, zero_division=0))\n\n# DECISION TREE CLASSIFIER\nprint(\"------------------------------------------------\")\nprint(\"Decision Tree Classifier\")\ndt_model = DecisionTreeClassifier(random_state=42)\ndt_model.fit(X_train, y_train)\ny_pred_dt = dt_model.predict(X_test)\ndt_accuracy = accuracy_score(y_test, y_pred_dt)\nprint(f\"Decision Tree Accuracy: {dt_accuracy}\")\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred_dt, zero_division=0))\n\n# LINEAR REGRESSION (For regression task, if 'generated' was numeric)\n# Note: Since `generated` is binary, we can use regression to explore probabilities but not as a primary classification method.\nprint(\"------------------------------------------------\")\nprint(\"Linear Regression\")\nlin_reg_model = Ridge(alpha=1.0, random_state=42)\nlin_reg_model.fit(X_train, y_train)\ny_pred_lr = lin_reg_model.predict(X_test)\nlr_mse = mean_squared_error(y_test, y_pred_lr)\nprint(f\"Linear Regression Mean Squared Error: {lr_mse}\")\n\n# Feature Importance from Random Forest (if applicable)\nif hasattr(rf_model, 'feature_importances_'):\n    feature_importance = rf_model.feature_importances_\n    feature_names = vectorizer.get_feature_names_out()\n    sorted_idx = np.argsort(feature_importance)[-10:]  # Top 10 features\n    print(\"\\nTop 10 Important Features (Random Forest):\")\n    for idx in sorted_idx[::-1]:\n        print(f\"{feature_names[idx]}: {feature_importance[idx]:.4f}\")\n\n# Plotting Feature Importance (optional)\nplt.figure(figsize=(10, 6))\nplt.barh([feature_names[idx] for idx in sorted_idx], feature_importance[sorted_idx])\nplt.title(\"Top 10 Features by Importance (Random Forest)\")\nplt.xlabel(\"Importance\")\nplt.ylabel(\"Feature\")\nplt.show()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}